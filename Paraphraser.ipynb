{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666a3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial\n",
    "from nltk.corpus import wordnet as wn\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25face82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'care'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pattern.en import conjugate, lemma, lexeme, PRESENT,PAST,FUTURE,PARTICIPLE, SG,tenses\n",
    "\n",
    "def get_verb_tense(verb):\n",
    "    if(PAST in tenses(verb)):\n",
    "        return PAST\n",
    "    if(PRESENT in tenses(verb)):\n",
    "        return PRESENT\n",
    "    if(PRESENT in tenses(verb)):\n",
    "        return PRESENT\n",
    "    if('progressive' in tenses(verb)[0][4]):\n",
    "        return PARTICIPLE\n",
    "\n",
    "\n",
    "# 2\n",
    "def my_conjugate(verb1, verb2_stem):\n",
    "    \n",
    "    verb1_stem = lemma(verb1)\n",
    "    if(verb1_stem == verb1):\n",
    "        return verb2_stem\n",
    "    verb1_tense = get_verb_tense(verb1)\n",
    "    verb2 = conjugate(verb=verb2_stem,tense=verb1_tense,number=SG)\n",
    "    return verb2\n",
    "    \n",
    "verb1 = \"like\"\n",
    "verb2_stem = \"care\"\n",
    "my_conjugate(verb1, verb2_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16b51e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this class is used to recognize the named entities in the sentence\n",
    "and encode them in a way that ensure they won't be replaced when paraphrasing\n",
    "'''\n",
    "class NerEncoder():\n",
    "    def __init__(self):\n",
    "        self.ner_dict = {}\n",
    "        self.label_index = 0\n",
    "        \n",
    "        \n",
    "    def translate(self, label, word):\n",
    "        self.label_index += 1\n",
    "        if label == 'NORP':\n",
    "            self.ner_dict[\"Group\" + str(self.label_index)] = word\n",
    "            return \"Group\" + str(self.label_index)\n",
    "        if label == 'FAC':\n",
    "            self.ner_dict[\"Place\" + str(self.label_index)] = word\n",
    "            return \"Place\" + str(self.label_index)\n",
    "        if label == 'ORG':\n",
    "            self.ner_dict[\"Organization\" + str(self.label_index)] = word\n",
    "            return \"Organization\" + str(self.label_index)\n",
    "        if label == 'GPE':\n",
    "            self.ner_dict[\"State\" + str(self.label_index)] = word\n",
    "            return \"State\" + str(self.label_index)\n",
    "        if label == 'LOC':\n",
    "            self.ner_dict[\"Location\" + str(self.label_index)] = word\n",
    "            return \"Location\" + str(self.label_index)\n",
    "        if label == 'PERCENT':\n",
    "            self.ner_dict[\"Percentage\" + str(self.label_index)] = word\n",
    "            return \"Percentage\" + str(self.label_index)\n",
    "        if label == 'ORDINAL':\n",
    "            self.ner_dict[\"Number\" + str(self.label_index)] = word\n",
    "            return \"Number\" + str(self.label_index)\n",
    "        if label == 'CARDINAL':\n",
    "            self.ner_dict[\"first\" + str(self.label_index)] = word\n",
    "            return \"first\" + str(self.label_index)\n",
    "        else:\n",
    "            self.ner_dict[label + str(self.label_index)] = word\n",
    "            return label + str(self.label_index)\n",
    "        \n",
    "    \n",
    "    def encode(self, doc):\n",
    "        res = \"\"\n",
    "        b = False\n",
    "        for i, token in enumerate(doc):\n",
    "            for ent in doc.ents:\n",
    "                if i == ent.start:\n",
    "                    b = True\n",
    "                    res += self.translate(ent.label_, ent.text) + \" \"\n",
    "                if i == ent.end:\n",
    "                    b = False\n",
    "            if b==False:\n",
    "                res += token.text +\" \"\n",
    "        return res\n",
    "    def decode(self, doc):\n",
    "        res = \"\"\n",
    "        for token in doc:\n",
    "            if token.text in self.ner_dict:\n",
    "                res += self.ner_dict[token.text] + \" \"\n",
    "            else:\n",
    "                res += token.text + \" \"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7872b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON1 loves Organization2 , he lives in State3 \n",
      "Tony Mark Smith loves Massachusetts Institute of Technology , he lives in the United States of America \n"
     ]
    }
   ],
   "source": [
    "#example of using NerEncoder\n",
    "doc = nlp(u'Tony Mark Smith loves Massachusetts Institute of Technology, he lives in the United States of America')\n",
    "encoder = NerEncoder()\n",
    "print(encoder.encode(doc))\n",
    "print(encoder.decode(nlp(encoder.encode(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21e1687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate \n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c0210e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b36e7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def pos_extractor(pos):\n",
    "    if pos == \"NOUN\":\n",
    "        return \"n\"\n",
    "    if pos == \"VERB\":\n",
    "        return \"v\"\n",
    "    if pos == \"ADJ\":\n",
    "        return \"a\"\n",
    "    if pos == \"ADV\":\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"o\"\n",
    "    \n",
    "    \n",
    "def get_index(sent, words):\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(w) for w in words]\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        if ps.stem(word) in words:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74fcca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_subsets(lst1, lst2):\n",
    "    res = []\n",
    "    for item1 in lst1:\n",
    "        for item2 in lst2:\n",
    "            f = item1.copy()\n",
    "            f.append(item2)\n",
    "            res.append(f)\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_paraphrase(to_be_replaced, encoder, max_num):\n",
    "    \"\"\"\n",
    "    this functions a list of lists, NER encoder, and max_num\n",
    "    the first parameter has a list for every word in the sentence containing the potential replacements with their similarity\n",
    "    as tuple, along with the original word with similarity of 1.\n",
    "    the second parameter is the NER encoder used, in order to decode the result.\n",
    "    the last one is the number of paraphrases wanted\n",
    "    \"\"\"\n",
    "    subsets = []\n",
    "    for i in range(len(to_be_replaced[0])):\n",
    "        subsets.append([i])\n",
    "    for i, replace in enumerate(to_be_replaced):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        a = [i for i in range(len(replace))]\n",
    "        subsets = all_subsets(subsets, a)\n",
    "    res = []\n",
    "    for subset in subsets:\n",
    "        text = \"\"\n",
    "        score = 1.0\n",
    "        for i, word_index in enumerate(subset):\n",
    "            text += to_be_replaced[i][word_index][0]\n",
    "            text += \" \"\n",
    "            score*=to_be_replaced[i][word_index][1]\n",
    "        sent = tuple((encoder.decode(nlp(text)), score))\n",
    "        res.append(sent)\n",
    "    sorted_list = sorted(res, key=lambda x: x[1], reverse = True) \n",
    "    return sorted_list[:min(max_num, len(sorted_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e004c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(doc):\n",
    "    \"\"\"\n",
    "    this function takes the sentence and outputs its paraphrases\n",
    "    \"\"\"\n",
    "    to_be_replaced = []\n",
    "    doc = nlp(encoder.encode(doc))\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(doc.text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    for i, token in enumerate(doc):\n",
    "        if pos_extractor(token.pos_) == \"o\":\n",
    "            to_be_replaced.append([tuple((token.text, 1.0))])\n",
    "            continue\n",
    "        if tokenized_text.count(token.text) > 0:\n",
    "            word_index = tokenized_text.index(token.text)\n",
    "            word_embedding = list_token_embeddings[word_index]\n",
    "        else:\n",
    "            to_be_replaced.append([tuple((token.text, 1.0))])\n",
    "            continue\n",
    "        synsets = wn.synsets(token.text, pos = pos_extractor(token.pos_))\n",
    "        to_be_appended = [tuple((token.text, 1.0))]\n",
    "        new_words = set(token.text)\n",
    "        new_words.add(token.lemma_)\n",
    "        for synset in synsets:\n",
    "            names = [x.name() for x in synset.lemmas()]\n",
    "            for name in names:\n",
    "                context = doc.text.split(' ')\n",
    "                context[i] = name\n",
    "                tokenized_text_example, tokens_tensor_example, segments_tensors_example = bert_text_preparation(str(context), tokenizer)\n",
    "                list_token_embeddings_example = get_bert_embeddings(tokens_tensor_example, segments_tensors_example, model)\n",
    "                if tokenized_text_example.count(name) > 0:\n",
    "                    word_index_example = tokenized_text_example.index(name)\n",
    "                    word_embedding_example = list_token_embeddings_example[word_index_example]\n",
    "                    if name not in new_words:\n",
    "                        new_words.add(name)\n",
    "                        if pos_extractor(token.pos_) == \"v\":\n",
    "                                to_be_appended.append(tuple((my_conjugate(token.text, name), 1 - cosine(word_embedding, word_embedding_example))))\n",
    "                        else:\n",
    "                                to_be_appended.append(tuple((name, 1 - cosine(word_embedding, word_embedding_example))))\n",
    "\n",
    "        to_be_replaced.append(to_be_appended)\n",
    "    result = generate_paraphrase(to_be_replaced, encoder, 10)\n",
    "    for res in result:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de8b0914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('he achieved his goal ', 1.0)\n",
      "('he achieved his destination ', 0.5151607832447769)\n",
      "('he accomplished his goal ', 0.5039548547197482)\n",
      "('he achieved his finish ', 0.4545435950979153)\n",
      "('he attained his goal ', 0.4398768661175332)\n",
      "('he achieved his end ', 0.43298626937261275)\n",
      "('he reached his goal ', 0.4313708732398621)\n",
      "('he accomplished his destination ', 0.2596177776774332)\n",
      "('he accomplished his finish ', 0.22906945143136195)\n",
      "('he attained his destination ', 0.22660731088036626)\n"
     ]
    }
   ],
   "source": [
    "#example1\n",
    "doc = nlp(\"he achieved his goal\")\n",
    "paraphrase(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b781b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('messi won the world cup to prove he is the best and end the debate ', 1.0)\n",
      "('messi won the world cup to prove he is the best and end the debate ', 0.6888724530837383)\n",
      "('messi won the world cup to prove he is the best and end the argument ', 0.5631689713651935)\n",
      "('messi won the world cup to test he is the best and end the debate ', 0.5400497893885152)\n",
      "('messi won the world cup to demonstrate he is the best and end the debate ', 0.5261544497239785)\n",
      "('messi won the world cup to show he is the best and end the debate ', 0.5243720718479813)\n",
      "('messi won the world cup to prove he is the best and stop the debate ', 0.5237512140363605)\n",
      "('messi won the world cup to prove he is the expert and end the debate ', 0.5012637257048709)\n",
      "('messi won the world cup to try he is the best and end the debate ', 0.49333191724091763)\n",
      "('messi won the world cup to establish he is the best and end the debate ', 0.49215984021086)\n"
     ]
    }
   ],
   "source": [
    "#example2\n",
    "doc = nlp(\"messi won the world cup to prove he is the best and end the debate\")\n",
    "paraphrase(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
